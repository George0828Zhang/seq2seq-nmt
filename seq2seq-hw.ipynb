{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下載和引入需要的函式庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user torch>=1.5.0 editdistance jieba-fast sacrebleu sacremoses sentencepiece tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/pytorch/fairseq.git\n",
    "# !cd fairseq && git checkout 9a1c497\n",
    "# !pip install --upgrade --user fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pprint\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import tqdm.auto as tqdm\n",
    "from pathlib import Path\n",
    "from argparse import Namespace\n",
    "from fairseq import utils\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    level=\"INFO\",\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "logger = logging.getLogger(\"seq2seq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:07:15 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2021-02-04 15:07:15 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 7.793 GB ; name = GeForce RTX 2070                        \n",
      "2021-02-04 15:07:15 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n"
     ]
    }
   ],
   "source": [
    "cuda_env = utils.CudaEnvironment()\n",
    "utils.CudaEnvironment.pretty_print_cuda_env_list([cuda_env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 實驗的參數設定表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Namespace( # common ?\n",
    "    datadir = \"./seq2seq-nmt/DATA/data-bin/ted2020\",\n",
    "    savedir = \"./checkpoints-transformer\",\n",
    "    source_lang = \"en\",\n",
    "    target_lang = \"zh\",\n",
    "    \n",
    "    # cpu threads when fetching & processing data.\n",
    "    num_workers=8,  \n",
    "    # batch size in terms of tokens. gradient accumulation increases the effective batchsize.\n",
    "    max_tokens=4096,\n",
    "    accum_steps=4,\n",
    "    # when calculating loss, normalized by number of sentences instead of number of tokens\n",
    "    sentence_average=False,\n",
    "    \n",
    "    # maximum epochs for training\n",
    "    max_epoch=40,\n",
    "    start_epoch=21,\n",
    "    \n",
    "    # beam size for beam search\n",
    "    beam=5, \n",
    "    # generate sequences of maximum length ax + b, where x is the source length\n",
    "    max_len_a=1.2, \n",
    "    max_len_b=10, \n",
    "    # length penalty: <1.0 favors shorter, >1.0 favors longer sentences\n",
    "    lenpen=1, \n",
    "    # when decoding, post process sentence by removing sentencepiece symbols and jieba tokenization.\n",
    "    post_process = \"sentencepiece\",\n",
    "    remove_jieba = True,\n",
    "    \n",
    "    # checkpoints\n",
    "    keep_last_epochs = 3,\n",
    "    resume=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 借用fairseq的TranslationTask\n",
    "* mmap dataset非常之快\n",
    "* 有現成的 dataloader iterator\n",
    "* 字典 task.source_dictionary 和 task.target_dictionary 也很好用 \n",
    "* 有實做 beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:07:15 | INFO | fairseq.tasks.translation | [en] dictionary: 8000 types\n",
      "2021-02-04 15:07:15 | INFO | fairseq.tasks.translation | [zh] dictionary: 8000 types\n"
     ]
    }
   ],
   "source": [
    "from fairseq.tasks.translation import TranslationConfig, TranslationTask\n",
    "\n",
    "## setup task\n",
    "task_cfg = TranslationConfig(\n",
    "    data=config.datadir,\n",
    "    source_lang=config.source_lang,\n",
    "    target_lang=config.target_lang,\n",
    "    train_subset=\"train\",\n",
    "    required_seq_len_multiple=8,\n",
    "    dataset_impl=\"mmap\",\n",
    ")\n",
    "task = TranslationTask.setup_task(task_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:07:15 | INFO | seq2seq | loading data for epoch 1\n",
      "2021-02-04 15:07:15 | INFO | fairseq.data.data_utils | loaded 335,785 examples from: ./seq2seq-nmt/DATA/data-bin/ted2020/train.en-zh.en\n",
      "2021-02-04 15:07:15 | INFO | fairseq.data.data_utils | loaded 335,785 examples from: ./seq2seq-nmt/DATA/data-bin/ted2020/train.en-zh.zh\n",
      "2021-02-04 15:07:15 | INFO | fairseq.tasks.translation | ./seq2seq-nmt/DATA/data-bin/ted2020 train en-zh 335785 examples\n",
      "2021-02-04 15:07:15 | INFO | fairseq.data.data_utils | loaded 3,426 examples from: ./seq2seq-nmt/DATA/data-bin/ted2020/valid.en-zh.en\n",
      "2021-02-04 15:07:15 | INFO | fairseq.data.data_utils | loaded 3,426 examples from: ./seq2seq-nmt/DATA/data-bin/ted2020/valid.en-zh.zh\n",
      "2021-02-04 15:07:15 | INFO | fairseq.tasks.translation | ./seq2seq-nmt/DATA/data-bin/ted2020 valid en-zh 3426 examples\n"
     ]
    }
   ],
   "source": [
    "# load data into task\n",
    "\n",
    "# num_workers=8\n",
    "# max_tokens=4096\n",
    "\n",
    "def load_data_iterator(task, split, epoch=1, max_tokens=1000, num_workers=1):\n",
    "    batch_iterator = task.get_batch_iterator(\n",
    "        dataset=task.dataset(split),\n",
    "        max_tokens=max_tokens,\n",
    "        max_sentences=None,\n",
    "        max_positions=utils.resolve_max_positions(\n",
    "            task.max_positions(),\n",
    "            max_tokens,\n",
    "        ),\n",
    "        ignore_invalid_inputs=True,\n",
    "        seed=138,\n",
    "        num_workers=num_workers,\n",
    "        epoch=epoch,\n",
    "    )\n",
    "    return batch_iterator\n",
    "logger.info(\"loading data for epoch 1\")\n",
    "task.load_dataset(split=\"train\", epoch=1)\n",
    "task.load_dataset(split=\"valid\", epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Smoothing Regularization\n",
    "* 讓模型學習輸出較不集中的分佈，防止模型過度自信"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothedCrossEntropyCriterion(nn.Module):\n",
    "    def __init__(self, smoothing, ignore_index=None, reduce=True):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reduce = reduce\n",
    "    \n",
    "    def forward(self, lprobs, target):\n",
    "        if target.dim() == lprobs.dim() - 1:\n",
    "            target = target.unsqueeze(-1)\n",
    "        nll_loss = -lprobs.gather(dim=-1, index=target)\n",
    "        smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
    "        if self.ignore_index is not None:\n",
    "            pad_mask = target.eq(self.ignore_index)\n",
    "            nll_loss.masked_fill_(pad_mask, 0.0)\n",
    "            smooth_loss.masked_fill_(pad_mask, 0.0)\n",
    "        else:\n",
    "            nll_loss = nll_loss.squeeze(-1)\n",
    "            smooth_loss = smooth_loss.squeeze(-1)\n",
    "        if self.reduce:\n",
    "            nll_loss = nll_loss.sum()\n",
    "            smooth_loss = smooth_loss.sum()\n",
    "        eps_i = self.smoothing / lprobs.size(-1)\n",
    "        loss = (1.0 - self.smoothing) * nll_loss + eps_i * smooth_loss\n",
    "        return loss\n",
    "    \n",
    "criterion = LabelSmoothedCrossEntropyCriterion(\n",
    "    smoothing=0.1,\n",
    "    ignore_index=task.target_dictionary.pad(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定義模型架構\n",
    "* 我們一樣繼承fairseq的encoder/decoder/model, 這樣測試階段才能直接用他寫好的beam search decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.models import (\n",
    "    FairseqEncoder, \n",
    "    FairseqDecoder,\n",
    "    FairseqIncrementalDecoder,\n",
    "    FairseqEncoderDecoderModel\n",
    ")\n",
    "from fairseq.models.lstm import AttentionLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNEncoder(FairseqEncoder):\n",
    "    def __init__(self, args, dictionary, embed_tokens):\n",
    "        super().__init__(dictionary)\n",
    "        self.embed_tokens = embed_tokens\n",
    "        \n",
    "        self.embed_dim = args.encoder_embed_dim\n",
    "        self.hidden_dim = args.encoder_ffn_embed_dim\n",
    "        self.num_layers = args.encoder_layers\n",
    "        \n",
    "        self.dropout_in_module = nn.Dropout(args.dropout)\n",
    "        self.rnn = nn.GRU(\n",
    "            self.embed_dim, \n",
    "            self.hidden_dim, \n",
    "            self.num_layers, \n",
    "            dropout=args.dropout, \n",
    "            batch_first=False, \n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout_out_module = nn.Dropout(args.dropout)\n",
    "        \n",
    "        self.padding_idx = dictionary.pad()\n",
    "        \n",
    "    def combine_bidir(self, outs, bsz: int):\n",
    "        out = outs.view(self.num_layers, 2, bsz, -1).transpose(1, 2).contiguous()\n",
    "        return out.view(self.num_layers, bsz, -1)\n",
    "\n",
    "    def forward(self, src_tokens, src_lengths, **unused):\n",
    "        bsz, seqlen = src_tokens.size()\n",
    "        \n",
    "        # get embeddings\n",
    "        x = self.embed_tokens(src_tokens)\n",
    "        x = self.dropout_in_module(x)\n",
    "\n",
    "        # B x T x C -> T x B x C\n",
    "        x = x.transpose(0, 1)\n",
    "        \n",
    "        # 過雙向RNN\n",
    "        state_size = 2 * self.num_layers, bsz, self.hidden_dim\n",
    "        h0 = x.new_zeros(*state_size)\n",
    "        x, final_hiddens = self.rnn(x, h0)\n",
    "        outputs = self.dropout_out_module(x)\n",
    "        # outputs = [sequence len, batch size, hid dim * directions]\n",
    "        # hidden =  [num_layers * directions, batch size  , hid dim]\n",
    "        # outputs 是最上層RNN的輸出\n",
    "        \n",
    "        # 因為 Encoder 是雙向的RNN，所以需要將同一層兩個方向的 hidden state 接在一起\n",
    "        final_hiddens = self.combine_bidir(final_hiddens, bsz)\n",
    "        # hidden =  [num_layers x batch x num_directions*hidden]\n",
    "        \n",
    "        encoder_padding_mask = src_tokens.eq(self.padding_idx).t()\n",
    "        return tuple(\n",
    "            (\n",
    "                outputs,  # seq_len x batch x hidden\n",
    "                final_hiddens,  # num_layers x batch x num_directions*hidden\n",
    "                encoder_padding_mask,  # seq_len x batch\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def reorder_encoder_out(self, encoder_out, new_order):\n",
    "        \"\"\" 這個beam search時會用到 \"\"\"\n",
    "        return tuple(\n",
    "            (\n",
    "                encoder_out[0].index_select(1, new_order),\n",
    "                encoder_out[1].index_select(1, new_order),\n",
    "                encoder_out[2].index_select(1, new_order),\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_embed_dim, source_embed_dim, output_embed_dim, bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_proj = nn.Linear(input_embed_dim, source_embed_dim, bias=bias)\n",
    "        self.output_proj = nn.Linear(\n",
    "            input_embed_dim + source_embed_dim, output_embed_dim, bias=bias\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, source_hids, encoder_padding_mask):\n",
    "        # inputs: T, B, dim\n",
    "        # source_hids: S x B x dim\n",
    "        # padding mask:  # S x B\n",
    "        \n",
    "        # convert all to batch first\n",
    "        inputs = inputs.transpose(1,0) # B, T, dim\n",
    "        source_hids = source_hids.permute(1, 2, 0) # B, dim, S\n",
    "        encoder_padding_mask = encoder_padding_mask.transpose(1,0) # B, S\n",
    "        \n",
    "        x = self.input_proj(inputs)\n",
    "\n",
    "        # 計算attention\n",
    "        # (B, T, S)\n",
    "        attn_scores = torch.bmm(x, source_hids)\n",
    "\n",
    "        # 擋住padding位置的attention\n",
    "        if encoder_padding_mask is not None:\n",
    "            # 利用broadcast  B, S -> (B, 1, S)\n",
    "            encoder_padding_mask = encoder_padding_mask.unsqueeze(1)\n",
    "            attn_scores = (\n",
    "                attn_scores.float()\n",
    "                .masked_fill_(encoder_padding_mask, float(\"-inf\"))\n",
    "                .type_as(attn_scores)\n",
    "            )  # FP16 support: cast to float and back\n",
    "\n",
    "        # (B, T, S), 並在source對應維度softmax\n",
    "        attn_scores = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # source_hids 形狀 (B, dim, S) -> (B, S, dim)\n",
    "        # 形狀 (B, T, S)(B, S, dim) -> (B, T, dim) 加權平均\n",
    "        x = torch.bmm(attn_scores, source_hids.permute(0, 2, 1))\n",
    "\n",
    "        # (B, T, dim)\n",
    "        x = torch.cat((x, inputs), dim=-1)\n",
    "        x = torch.tanh(self.output_proj(x)) # concat + linear\n",
    "        \n",
    "        # 回復形狀 (B, T, dim) -> (T, B, dim)\n",
    "        return x.transpose(1,0), attn_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder 解碼器\n",
    "* 解碼器的hidden states會用編碼器最後的hidden state來初始化\n",
    "* 解碼器同時也根據目前timestep的輸入(也就是之前timestep的output)，改變hidden states，並輸出結果 \n",
    "* 如果加入attention可以使表現更好\n",
    "* 我們把步驟寫在Decoder裡好讓等等Seq2seq這個型別可以通用RNN和Transformer而不用改寫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "class RNNDecoder(FairseqIncrementalDecoder):\n",
    "    def __init__(self, args, dictionary, embed_tokens):\n",
    "        super().__init__(dictionary)\n",
    "        self.embed_tokens = embed_tokens\n",
    "                \n",
    "        self.embed_dim = args.decoder_embed_dim\n",
    "        self.hidden_dim = args.encoder_ffn_embed_dim*2\n",
    "        logger.warning(\"decoder ffn dimension is automatically \"\n",
    "                       f\"scaled to 2*encoder_ffn_embed_dim: {self.hidden_dim}!\")\n",
    "        self.num_layers = args.decoder_layers\n",
    "        \n",
    "        self.dropout_in_module = nn.Dropout(args.dropout)\n",
    "        self.rnn = nn.GRU(\n",
    "            self.embed_dim, \n",
    "            self.hidden_dim, \n",
    "            self.num_layers, \n",
    "            dropout=args.dropout, \n",
    "            batch_first=False, \n",
    "            bidirectional=False\n",
    "        )\n",
    "#         self.attention = AttentionLayer(\n",
    "#             self.embed_dim, self.hidden_dim, self.embed_dim, bias=False\n",
    "#         )\n",
    "        self.attention = None\n",
    "        self.dropout_out_module = nn.Dropout(args.dropout)\n",
    "        \n",
    "        if self.hidden_dim != self.embed_dim:\n",
    "            self.project_out_dim = nn.Linear(self.hidden_dim, self.embed_dim)\n",
    "        else:\n",
    "            self.project_out_dim = None\n",
    "        \n",
    "        if args.share_decoder_input_output_embed:\n",
    "            self.output_projection = nn.Linear(\n",
    "                self.embed_tokens.weight.shape[1],\n",
    "                self.embed_tokens.weight.shape[0],\n",
    "                bias=False,\n",
    "            )\n",
    "            self.output_projection.weight = self.embed_tokens.weight\n",
    "        else:\n",
    "            self.output_projection = nn.Linear(\n",
    "                self.output_embed_dim, len(dictionary), bias=False\n",
    "            )\n",
    "            nn.init.normal_(\n",
    "                self.output_projection.weight, mean=0, std=self.output_embed_dim ** -0.5\n",
    "            )\n",
    "        \n",
    "    def forward(self, prev_output_tokens, encoder_out, incremental_state=None, **unused):\n",
    "        # 取出encoder的輸出\n",
    "        encoder_outputs, encoder_hiddens, encoder_padding_mask = encoder_out\n",
    "        # outputs:          seq_len x batch x num_directions*hidden\n",
    "        # encoder_hiddens:  num_layers x batch x num_directions*encoder_hidden\n",
    "        # padding_mask:     seq_len x batch\n",
    "        \n",
    "        if incremental_state is not None and len(incremental_state) > 0:\n",
    "            # 有上個timestep留下的資訊，讀進來就可以繼續decode，不用從bos重來\n",
    "            prev_output_tokens = prev_output_tokens[:, -1:]\n",
    "            cache_state = self.get_incremental_state(incremental_state, \"cached_state\")\n",
    "            prev_hiddens = cache_state[\"prev_hiddens\"]\n",
    "        else:\n",
    "            # 沒有incremental state代表這是training或者是decode時的第一步\n",
    "            # 準備seq2seq: 把encoder_hiddens pass進去decoder的hidden states\n",
    "            prev_hiddens = encoder_hiddens\n",
    "        \n",
    "        bsz, seqlen = prev_output_tokens.size()\n",
    "        \n",
    "        # embed tokens\n",
    "        x = self.embed_tokens(prev_output_tokens)\n",
    "        x = self.dropout_in_module(x)\n",
    "\n",
    "        # B x T x C -> T x B x C\n",
    "        x = x.transpose(0, 1)\n",
    "                \n",
    "        # 做decoder-to-encoder attention\n",
    "        if self.attention is not None:\n",
    "            x, attn = self.attention(x, encoder_outputs, encoder_padding_mask)\n",
    "                        \n",
    "        # 過單向RNN\n",
    "        x, final_hiddens = self.rnn(x, prev_hiddens)\n",
    "        # outputs = [sequence len, batch size, hid dim]\n",
    "        # hidden =  [num_layers * directions, batch size  , hid dim]\n",
    "        x = self.dropout_out_module(x)\n",
    "                \n",
    "        # 投影到embedding size (如果hidden 和embed size不一樣，然後share_embedding又設成True,需要額外project一次)\n",
    "        if self.project_out_dim != None:\n",
    "            x = self.project_out_dim(x)\n",
    "        \n",
    "        # 投影到vocab size 的分佈\n",
    "        x = self.output_projection(x)\n",
    "        \n",
    "        # T x B x C -> B x T x C\n",
    "        x = x.transpose(1, 0)\n",
    "        \n",
    "        # 如果是Incremental, 記錄這個timestep的hidden states, 下個timestep讀回來\n",
    "        cache_state = {\n",
    "            \"prev_hiddens\": final_hiddens,\n",
    "        }\n",
    "        self.set_incremental_state(incremental_state, \"cached_state\", cache_state)\n",
    "        \n",
    "        return x, None\n",
    "    \n",
    "    def reorder_incremental_state(\n",
    "        self,\n",
    "        incremental_state,\n",
    "        new_order,\n",
    "    ):\n",
    "        \"\"\" 這個beam search時會用到 \"\"\"\n",
    "        cache_state = self.get_incremental_state(incremental_state, \"cached_state\")\n",
    "        prev_hiddens = cache_state[\"prev_hiddens\"]\n",
    "        prev_hiddens = [p.index_select(0, new_order) for p in prev_hiddens]\n",
    "        cache_state = {\n",
    "            \"prev_hiddens\": torch.stack(prev_hiddens),\n",
    "        }\n",
    "        self.set_incremental_state(incremental_state, \"cached_state\", cache_state)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.models.transformer import (\n",
    "    TransformerEncoder, \n",
    "    TransformerDecoder,\n",
    "    TransformerModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(FairseqEncoderDecoderModel):\n",
    "    def __init__(self, args, encoder, decoder):\n",
    "        super().__init__(encoder, decoder)\n",
    "        self.args = args\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        src_tokens,\n",
    "        src_lengths,\n",
    "        prev_output_tokens,\n",
    "        return_all_hiddens: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Run the forward pass for an encoder-decoder model.\n",
    "        \"\"\"\n",
    "        encoder_out = self.encoder(\n",
    "            src_tokens, src_lengths=src_lengths, return_all_hiddens=return_all_hiddens\n",
    "        )\n",
    "        logits, extra = self.decoder(\n",
    "            prev_output_tokens,\n",
    "            encoder_out=encoder_out,\n",
    "            src_lengths=src_lengths,\n",
    "            return_all_hiddens=return_all_hiddens,\n",
    "        )\n",
    "        return logits, extra\n",
    "\n",
    "\n",
    "def build_model(args, task):\n",
    "    \"\"\" 按照參數設定建置模型 \"\"\"\n",
    "    src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n",
    "\n",
    "    # 詞嵌入\n",
    "    encoder_embed_tokens = nn.Embedding(len(src_dict), args.encoder_embed_dim, src_dict.pad())\n",
    "    decoder_embed_tokens = nn.Embedding(len(tgt_dict), args.decoder_embed_dim, tgt_dict.pad())\n",
    "    \n",
    "    # 編碼器與解碼器\n",
    "    encoder = TransformerEncoder(args, src_dict, encoder_embed_tokens)\n",
    "    decoder = TransformerDecoder(args, tgt_dict, decoder_embed_tokens)\n",
    "    \n",
    "    # 序列到序列模型\n",
    "    model = Seq2Seq(args, encoder, decoder)\n",
    "    \n",
    "    # 序列到序列模型的初始化很重要 需要特別處理\n",
    "    def init_params(module):\n",
    "        from fairseq.modules import MultiheadAttention\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        if isinstance(module, MultiheadAttention):\n",
    "            module.q_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            module.k_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            module.v_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if isinstance(module, nn.RNNBase):\n",
    "            for name, param in module.named_parameters():\n",
    "                if \"weight\" in name or \"bias\" in name:\n",
    "                    param.data.uniform_(-0.1, 0.1)\n",
    "            \n",
    "    # 初始化模型\n",
    "    model.apply(init_params)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 設定模型相關參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.models.transformer import base_architecture \n",
    "arch_args = Namespace(\n",
    "    encoder_embed_dim=128,\n",
    "    encoder_ffn_embed_dim=128,\n",
    "    encoder_layers=3,\n",
    "    encoder_attention_heads=4,\n",
    "    encoder_normalize_before=True,\n",
    "    decoder_embed_dim=128,\n",
    "    decoder_ffn_embed_dim=128,\n",
    "    decoder_layers=3,\n",
    "    decoder_attention_heads=4,\n",
    "    decoder_normalize_before=True,\n",
    "    share_decoder_input_output_embed=True,\n",
    "    dropout=0.1,\n",
    "    attention_dropout=0.1,\n",
    "    activation_dropout=0.1,\n",
    "    activation_fn=\"relu\",\n",
    "    max_source_positions=1024,\n",
    "    max_target_positions=1024,\n",
    ")\n",
    "\n",
    "# set default params\n",
    "base_architecture(arch_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(arch_args, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:07:15 | INFO | seq2seq | Seq2Seq(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(8000, 128, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(8000, 128, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (output_projection): Linear(in_features=128, out_features=8000, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "logger.info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device=device)\n",
    "criterion = criterion.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.data import iterators\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# sentence_average = False\n",
    "def train_one_epoch(epoch_itr, model, task, criterion, optimizer, accum_steps=1):\n",
    "    itr = epoch_itr.next_epoch_itr(shuffle=True)\n",
    "    itr = iterators.GroupedIterator(itr, accum_steps)\n",
    "    \n",
    "    stats = {\"loss\": []}\n",
    "    # automatic mixed precision (amp)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    model.train()\n",
    "    progress = tqdm.tqdm(itr, desc=f\"train epoch {epoch_itr.epoch}\", leave=False)\n",
    "    for samples in progress:\n",
    "        model.zero_grad()\n",
    "        sample_size = 0\n",
    "        for i, sample in enumerate(samples):\n",
    "            if i == 1:\n",
    "                # emptying the CUDA cache after the first step can reduce the chance of OOM\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            \"\"\"gradient accumulation\"\"\"\n",
    "            sample = utils.move_to_cuda(sample, device=device)\n",
    "            target = sample[\"target\"]\n",
    "            sample_size_i = sample[\"nsentences\"] if config.sentence_average else sample[\"ntokens\"]\n",
    "            sample_size += sample_size_i\n",
    "            \n",
    "            with autocast():\n",
    "                net_output = model.forward(**sample[\"net_input\"])\n",
    "                lprobs = F.log_softmax(net_output[0], -1)            \n",
    "                loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1))\n",
    "                \n",
    "                # logging\n",
    "                loss_print = loss.item()/sample_size_i\n",
    "                stats[\"loss\"].append(loss_print)\n",
    "                progress.set_postfix(loss=loss_print)\n",
    "                \n",
    "                scaler.scale(loss).backward()                \n",
    "        \n",
    "        scaler.unscale_(optimizer)\n",
    "        optimizer.multiply_grads(1 / (sample_size or 1.0)) # (sample_size or 1.0) handles the case of a zero gradient\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "    loss_print = np.mean(stats[\"loss\"])\n",
    "    logger.info(f\"training loss:\\t{loss_print:.4f}\")\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbatch = {\\n    \"id\": id,\\n    \"nsentences\": len(samples),\\n    \"ntokens\": ntokens,\\n    \"net_input\": {\\n        \"src_tokens\": src_tokens,\\n        \"src_lengths\": src_lengths,\\n        \"prev_output_tokens\": prev_output_tokens,\\n    },\\n    \"target\": target,\\n}\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "batch = {\n",
    "    \"id\": id,\n",
    "    \"nsentences\": len(samples),\n",
    "    \"ntokens\": ntokens,\n",
    "    \"net_input\": {\n",
    "        \"src_tokens\": src_tokens,\n",
    "        \"src_lengths\": src_lengths,\n",
    "        \"prev_output_tokens\": prev_output_tokens,\n",
    "    },\n",
    "    \"target\": target,\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# post_process = \"sentencepiece\"\n",
    "# remove_jieba = True\n",
    "\n",
    "def decode(toks, dictionary, escape_unk=False, remove_jieba=False):\n",
    "    s = dictionary.string(\n",
    "        toks.int().cpu(),\n",
    "        config.post_process,\n",
    "        # The default unknown string in fairseq is `<unk>`, but\n",
    "        # this is tokenized by sacrebleu as `< unk >`, inflating\n",
    "        # BLEU scores. Instead, we use a somewhat more verbose\n",
    "        # alternative that is unlikely to appear in the real\n",
    "        # reference, but doesn't get split into multiple tokens.\n",
    "        unk_string=(\"UNKNOWNTOKENINREF\" if escape_unk else \"UNKNOWNTOKENINHYP\"),\n",
    "    )\n",
    "    # join into a sentence\n",
    "    if remove_jieba:\n",
    "        s = s.replace(\" \", \"\").replace(\"\\u2582\", \" \")\n",
    "    return s if s else \"UNKNOWNTOKENINHYP\"\n",
    "\n",
    "def inference_step(generator, sample, model):\n",
    "    gen_out = generator.generate([model], sample)\n",
    "    srcs = []\n",
    "    hyps = []\n",
    "    refs = []\n",
    "    for i in range(len(gen_out)):\n",
    "        \"\"\"for each example, collect the source, reference \n",
    "        and most probable hypothesis (index 0) 's tokens, \n",
    "        \"\"\"\n",
    "        srcs.append(decode(\n",
    "            utils.strip_pad(sample[\"net_input\"][\"src_tokens\"][i], task.source_dictionary.pad()), \n",
    "            task.source_dictionary,\n",
    "            remove_jieba=(config.source_lang == \"zh\") and config.remove_jieba,\n",
    "        ))\n",
    "        hyps.append(decode(\n",
    "            gen_out[i][0][\"tokens\"], \n",
    "            task.target_dictionary,\n",
    "            remove_jieba=(config.target_lang == \"zh\") and config.remove_jieba,\n",
    "        ))\n",
    "        refs.append(decode(\n",
    "            utils.strip_pad(sample[\"target\"][i], task.target_dictionary.pad()), \n",
    "            task.target_dictionary, \n",
    "            escape_unk=True,  # don't count <unk> as matches to the hypo\n",
    "            remove_jieba=(config.target_lang == \"zh\") and config.remove_jieba,\n",
    "        ))\n",
    "    return srcs, hyps, refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import sacrebleu\n",
    "from fairseq.dataclass.configs import GenerationConfig\n",
    "# gen_args = GenerationConfig(beam=5, max_len_a=1.2, max_len_b=10)\n",
    "sequence_generator = task.build_generator([model], config) #gen_args)\n",
    "\n",
    "def validate(model, task, criterion):\n",
    "    logger.info('begin validation')\n",
    "    itr = load_data_iterator(task, \"valid\", 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n",
    "    \n",
    "    stats = {\"loss\":[], \"bleu\": 0, \"srcs\":[], \"hyps\":[], \"refs\":[]}\n",
    "    srcs = []\n",
    "    hyps = []\n",
    "    refs = []\n",
    "    \n",
    "    model.eval()\n",
    "    progress = tqdm.tqdm(itr, desc=f\"validation\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(progress):\n",
    "            # validation loss\n",
    "            sample = utils.move_to_cuda(sample, device=device)\n",
    "            net_output = model.forward(**sample[\"net_input\"])\n",
    "\n",
    "            lprobs = F.log_softmax(net_output[0], -1)\n",
    "            target = sample[\"target\"]\n",
    "            sample_size = sample[\"nsentences\"] if config.sentence_average else sample[\"ntokens\"]\n",
    "            loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1)) / sample_size\n",
    "            progress.set_postfix(valid_loss=loss.item())\n",
    "            stats[\"loss\"].append(loss)\n",
    "            \n",
    "            # compute bleu\n",
    "            s, h, r = inference_step(sequence_generator, sample, model)\n",
    "            srcs.extend(s)\n",
    "            hyps.extend(h)\n",
    "            refs.extend(r)\n",
    "            \n",
    "    stats[\"loss\"] = torch.stack(stats[\"loss\"]).mean().item()\n",
    "    stats[\"bleu\"] = sacrebleu.corpus_bleu(hyps, [refs], tokenize='zh')\n",
    "    stats[\"srcs\"] = srcs\n",
    "    stats[\"hyps\"] = hyps\n",
    "    stats[\"refs\"] = refs\n",
    "    \n",
    "    showid = np.random.randint(len(hyps))\n",
    "    logger.info(\"example source: \" + srcs[showid])\n",
    "    logger.info(\"example hypothesis: \" + hyps[showid])\n",
    "    logger.info(\"example reference: \" + refs[showid])\n",
    "    \n",
    "    # show bleu results\n",
    "    logger.info(f\"validation loss:\\t{stats['loss']:.4f}\")\n",
    "    logger.info(stats[\"bleu\"].format())\n",
    "    return stats\n",
    "\n",
    "\n",
    "def validate_and_save(model, task, criterion, epoch, save=True):\n",
    "    \n",
    "    stats = validate(model, task, criterion)\n",
    "    bleu = stats['bleu']\n",
    "    loss = stats['loss']\n",
    "    \n",
    "    if save:\n",
    "        # save epoch checkpoints\n",
    "        savedir = Path(config.savedir).absolute()\n",
    "        savedir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        check = {\n",
    "            \"model\": model.state_dict(),\n",
    "            \"stats\": {\"bleu\": bleu.score, \"loss\": loss}\n",
    "        }\n",
    "        torch.save(check, savedir/f\"checkpoint{epoch}.pt\")\n",
    "        shutil.copy(savedir/f\"checkpoint{epoch}.pt\", savedir/f\"checkpoint_last.pt\")\n",
    "        logger.info(f\"saved epoch checkpoint:\\t{savedir}/checkpoint{epoch}.pt\")\n",
    "    \n",
    "        # save epoch samples\n",
    "        with open(savedir/f\"samples{epoch}.{config.source_lang}-{config.target_lang}.txt\", \"w\") as f:\n",
    "            for s, h in zip(stats[\"srcs\"], stats[\"hyps\"]):\n",
    "                f.write(f\"{s}\\t{h}\\n\")\n",
    "\n",
    "        # get best valid bleu    \n",
    "        if getattr(validate_and_save, \"best_bleu\", 0) < bleu.score:\n",
    "            validate_and_save.best_bleu = bleu.score\n",
    "            torch.save(check, savedir/f\"checkpoint_best.pt\")\n",
    "            \n",
    "        del_file = savedir / f\"checkpoint{epoch - config.keep_last_epochs}.pt\"\n",
    "        if del_file.exists():\n",
    "            del_file.unlink()\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def try_load_checkpoint(model, name=None):\n",
    "    name = name if name else \"checkpoint_last.pt\"\n",
    "    checkpath = Path(config.savedir)/name\n",
    "    if checkpath.exists():\n",
    "        check = torch.load(checkpath)\n",
    "        model.load_state_dict(check[\"model\"])\n",
    "        stats = check[\"stats\"]\n",
    "        logger.info(f\"loaded checkpoint {checkpath}:\\tloss={stats['loss']}\\tbleu={stats['bleu']}\")\n",
    "    else:\n",
    "        logger.info(f\"no checkpoints found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "    \n",
    "    @property\n",
    "    def param_groups(self):\n",
    "        return self.optimizer.param_groups\n",
    "        \n",
    "    def multiply_grads(self, c):\n",
    "        \"\"\"Multiplies grads by a constant *c*.\"\"\"                \n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    p.grad.data.mul_(c)\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = NoamOpt(\n",
    "    model_size=arch_args.encoder_embed_dim, \n",
    "    factor=0.5, \n",
    "    warmup=4000, \n",
    "    optimizer=torch.optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.0001))\n",
    "plt.plot(np.arange(1, 20000), [optimizer.rate(i) for i in range(1, 20000)])\n",
    "plt.legend([f\"{optimizer.model_size}:{optimizer.warmup}\"])\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb  4 15:07:17 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 455.45.01    Driver Version: 455.45.01    CUDA Version: 11.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 2070    On   | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| 17%   45C    P2    20W / 175W |    966MiB /  7979MiB |      4%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce GTX 1080    On   | 00000000:03:00.0  On |                  N/A |\r\n",
      "| 28%   33C    P8     6W / 180W |    362MiB /  8119MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1215      G   /usr/lib/xorg/Xorg                  4MiB |\r\n",
      "|    0   N/A  N/A      1928      G   /usr/lib/xorg/Xorg                  4MiB |\r\n",
      "|    0   N/A  N/A    457774      C   .../envs/mltatest/bin/python      953MiB |\r\n",
      "|    1   N/A  N/A      1215      G   /usr/lib/xorg/Xorg                 59MiB |\r\n",
      "|    1   N/A  N/A      1928      G   /usr/lib/xorg/Xorg                233MiB |\r\n",
      "|    1   N/A  N/A      2043      G   /usr/bin/gnome-shell               22MiB |\r\n",
      "|    1   N/A  N/A      3676      G   ...AAAAAAAAA= --shared-files        9MiB |\r\n",
      "|    1   N/A  N/A      5531      G   ...AAAAAAAA== --shared-files       20MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:07:17 | INFO | seq2seq | task: TranslationTask\n",
      "2021-02-04 15:07:17 | INFO | seq2seq | model: Seq2Seq\n",
      "2021-02-04 15:07:17 | INFO | seq2seq | criterion: LabelSmoothedCrossEntropyCriterion\n",
      "2021-02-04 15:07:17 | INFO | seq2seq | optimizer: NoamOpt\n",
      "2021-02-04 15:07:17 | INFO | seq2seq | num. model params: 2,844,928 (num. trained: 2,844,928)\n",
      "2021-02-04 15:07:17 | INFO | seq2seq | max tokens per batch = 4096, accumulate steps = 4\n",
      "2021-02-04 15:07:17 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[283679]\n",
      "2021-02-04 15:07:17 | INFO | seq2seq | no checkpoint found!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 21:   0%|          | 0/660 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:08:39 | INFO | seq2seq | training loss:\t8.0312\n",
      "2021-02-04 15:08:39 | INFO | seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:09:01 | INFO | seq2seq | example source: i keep telling my superego to back off and let me enjoy what i still have.\n",
      "2021-02-04 15:09:01 | INFO | seq2seq | example hypothesis: UNKNOWNTOKENINHYP\n",
      "2021-02-04 15:09:01 | INFO | seq2seq | example reference: 我總告訴自己的超我意識退下且讓我享受僅存的時間。\n",
      "2021-02-04 15:09:01 | INFO | seq2seq | validation loss:\t7.1266\n",
      "2021-02-04 15:09:01 | INFO | seq2seq | BLEU = 0.00 0.0/0.0/0.0/0.0 (BP = 0.000 ratio = 0.035 hyp_len = 3426 ref_len = 96725)\n",
      "2021-02-04 15:09:01 | INFO | seq2seq | saved epoch checkpoint:\t/home/george/Projects/mlta2021/checkpoints-transformer/checkpoint21.pt\n",
      "2021-02-04 15:09:01 | INFO | seq2seq | end of epoch 21\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 22:   0%|          | 0/660 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:10:23 | INFO | seq2seq | training loss:\t6.8430\n",
      "2021-02-04 15:10:23 | INFO | seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:10:46 | INFO | seq2seq | example source: in october 2003, over 6,000 runners from 49 different nationalities came to the start line, all determined, and when the gunfire went off, this time it was a signal to run in harmony, for a change.\n",
      "2021-02-04 15:10:46 | INFO | seq2seq | example hypothesis: 大大大大大大的,大大大大大大大,大大大大大大的的的的大大大大大大的的的。\n",
      "2021-02-04 15:10:46 | INFO | seq2seq | example reference: 在2003年10月,超過六千名跑者從49個不同的國家來到這條起跑線前,堅定不移,當起跑的槍聲響起,這一次是為了和睦而奔跑,帶來改變的契機。\n",
      "2021-02-04 15:10:46 | INFO | seq2seq | validation loss:\t6.4470\n",
      "2021-02-04 15:10:46 | INFO | seq2seq | BLEU = 0.28 8.2/1.1/0.1/0.0 (BP = 1.000 ratio = 1.288 hyp_len = 124570 ref_len = 96725)\n",
      "2021-02-04 15:10:46 | INFO | seq2seq | saved epoch checkpoint:\t/home/george/Projects/mlta2021/checkpoints-transformer/checkpoint22.pt\n",
      "2021-02-04 15:10:46 | INFO | seq2seq | end of epoch 22\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 23:   0%|          | 0/660 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:12:08 | INFO | seq2seq | training loss:\t6.1932\n",
      "2021-02-04 15:12:08 | INFO | seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:12:29 | INFO | seq2seq | example source: but we do have an unprecedented opportunity to try, and i believe we also, as adults, have a responsibility to do this.\n",
      "2021-02-04 15:12:29 | INFO | seq2seq | example hypothesis: 但,我們需要有有有有有的,我們需要做,我們需要的,我們需要不見了。\n",
      "2021-02-04 15:12:29 | INFO | seq2seq | example reference: 但這個機會前所未有,值得我們一試,我也認為,我們成人有責任要這麼做。\n",
      "2021-02-04 15:12:29 | INFO | seq2seq | validation loss:\t5.7531\n",
      "2021-02-04 15:12:29 | INFO | seq2seq | BLEU = 1.52 19.7/4.0/0.9/0.2 (BP = 0.761 ratio = 0.786 hyp_len = 75980 ref_len = 96725)\n",
      "2021-02-04 15:12:29 | INFO | seq2seq | saved epoch checkpoint:\t/home/george/Projects/mlta2021/checkpoints-transformer/checkpoint23.pt\n",
      "2021-02-04 15:12:29 | INFO | seq2seq | end of epoch 23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 24:   0%|          | 0/660 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:13:51 | INFO | seq2seq | training loss:\t5.6899\n",
      "2021-02-04 15:13:51 | INFO | seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:14:12 | INFO | seq2seq | example source: so because of these beauties -- the afsluitdijk: 32 kilometers, built by hand in 1932 -- we live with the water, we fight with the water, we try to find harmony, but sometimes we forget.\n",
      "2021-02-04 15:14:12 | INFO | seq2seq | example hypothesis: 因為這些問題是,80000000000000000年,我們在水水裡,我們開始在水水裡,我們開始發現了。\n",
      "2021-02-04 15:14:12 | INFO | seq2seq | example reference: 因為這些美──阿夫魯戴克大堤:32公里,1932年徒手建立──我們與水同住,我們與水相爭,我們試著找到和諧,但有時,我們會忘記。\n",
      "2021-02-04 15:14:12 | INFO | seq2seq | validation loss:\t5.3654\n",
      "2021-02-04 15:14:12 | INFO | seq2seq | BLEU = 4.64 30.3/10.1/3.7/1.3 (BP = 0.748 ratio = 0.775 hyp_len = 74974 ref_len = 96725)\n",
      "2021-02-04 15:14:12 | INFO | seq2seq | saved epoch checkpoint:\t/home/george/Projects/mlta2021/checkpoints-transformer/checkpoint24.pt\n",
      "2021-02-04 15:14:12 | INFO | seq2seq | end of epoch 24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 25:   0%|          | 0/660 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:15:34 | INFO | seq2seq | training loss:\t5.3815\n",
      "2021-02-04 15:15:34 | INFO | seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:15:54 | INFO | seq2seq | example source: and if the story is a good one, it might even make us smile.\n",
      "2021-02-04 15:15:54 | INFO | seq2seq | example hypothesis: 如果故事是好好,可能是我們會把我們做好。\n",
      "2021-02-04 15:15:54 | INFO | seq2seq | example reference: 而且如果這個故事好聽我們甚至會隨之微笑\n",
      "2021-02-04 15:15:54 | INFO | seq2seq | validation loss:\t5.0762\n",
      "2021-02-04 15:15:54 | INFO | seq2seq | BLEU = 8.21 36.4/14.1/5.9/2.5 (BP = 0.883 ratio = 0.889 hyp_len = 86018 ref_len = 96725)\n",
      "2021-02-04 15:15:54 | INFO | seq2seq | saved epoch checkpoint:\t/home/george/Projects/mlta2021/checkpoints-transformer/checkpoint25.pt\n",
      "2021-02-04 15:15:54 | INFO | seq2seq | end of epoch 25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 26:   0%|          | 0/660 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:17:16 | INFO | seq2seq | training loss:\t5.1709\n",
      "2021-02-04 15:17:16 | INFO | seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:17:37 | INFO | seq2seq | example source: that doesn’t mean the test itself is worthless— in fact, it does a good job of measuring the reasoning and problem-solving skills it sets out to.\n",
      "2021-02-04 15:17:37 | INFO | seq2seq | example hypothesis: 這不是說的測試,無論是無限的,事實上,事實是很好的工作,解決問題和問題的,解決方案,解決方案的解決方案。\n",
      "2021-02-04 15:17:37 | INFO | seq2seq | example reference: 那並不表示測驗本身毫無價值——事實上,它原本的目的是要測量推理和解決問題的技能,而在這方面它很成功。\n",
      "2021-02-04 15:17:37 | INFO | seq2seq | validation loss:\t4.8875\n",
      "2021-02-04 15:17:37 | INFO | seq2seq | BLEU = 10.19 36.5/15.1/6.8/3.1 (BP = 0.979 ratio = 0.980 hyp_len = 94747 ref_len = 96725)\n",
      "2021-02-04 15:17:37 | INFO | seq2seq | saved epoch checkpoint:\t/home/george/Projects/mlta2021/checkpoints-transformer/checkpoint26.pt\n",
      "2021-02-04 15:17:37 | INFO | seq2seq | end of epoch 26\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 27:   0%|          | 0/660 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:18:59 | INFO | seq2seq | training loss:\t5.0110\n",
      "2021-02-04 15:18:59 | INFO | seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:19:19 | INFO | seq2seq | example source: and yet, for someone who's apparently done this already, i still haven't figured anything out yet.\n",
      "2021-02-04 15:19:19 | INFO | seq2seq | example hypothesis: 然而,在某人身上,我還沒辦法做什麼,我仍然不知道任何任何東西。\n",
      "2021-02-04 15:19:19 | INFO | seq2seq | example reference: 雖然我曾經出生過一次我還是什麼都不了解\n",
      "2021-02-04 15:19:19 | INFO | seq2seq | validation loss:\t4.7301\n",
      "2021-02-04 15:19:19 | INFO | seq2seq | BLEU = 11.58 41.6/18.1/8.5/4.1 (BP = 0.907 ratio = 0.911 hyp_len = 88141 ref_len = 96725)\n",
      "2021-02-04 15:19:19 | INFO | seq2seq | saved epoch checkpoint:\t/home/george/Projects/mlta2021/checkpoints-transformer/checkpoint27.pt\n",
      "2021-02-04 15:19:19 | INFO | seq2seq | end of epoch 27\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 28:   0%|          | 0/660 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:20:40 | INFO | seq2seq | training loss:\t4.8899\n",
      "2021-02-04 15:20:40 | INFO | seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:21:01 | INFO | seq2seq | example source: the evolutionary role of the vulture is to rid the earth of harmful toxins produced following death.\n",
      "2021-02-04 15:21:01 | INFO | seq2seq | example hypothesis: 溫度的溫度是地球的火箭,因因因因因因素產生死亡。\n",
      "2021-02-04 15:21:01 | INFO | seq2seq | example reference: 禿鷲的進化作用是消除死屍產生的有害毒素。\n",
      "2021-02-04 15:21:01 | INFO | seq2seq | validation loss:\t4.6297\n",
      "2021-02-04 15:21:01 | INFO | seq2seq | BLEU = 12.60 42.1/18.8/9.1/4.6 (BP = 0.936 ratio = 0.938 hyp_len = 90714 ref_len = 96725)\n",
      "2021-02-04 15:21:02 | INFO | seq2seq | saved epoch checkpoint:\t/home/george/Projects/mlta2021/checkpoints-transformer/checkpoint28.pt\n",
      "2021-02-04 15:21:02 | INFO | seq2seq | end of epoch 28\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 29:   0%|          | 0/660 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:22:23 | INFO | seq2seq | training loss:\t4.8038\n",
      "2021-02-04 15:22:23 | INFO | seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:22:44 | INFO | seq2seq | example source: they moved into one of our family apartments and treasured each day that they had, which were far too few.\n",
      "2021-02-04 15:22:44 | INFO | seq2seq | example hypothesis: 他們搬到一家家家家家的家園,每天都走到他們,這太多了。\n",
      "2021-02-04 15:22:44 | INFO | seq2seq | example reference: 他們搬進我們家庭套房,並且珍惜每一天,但是時間實在太少了。\n",
      "2021-02-04 15:22:44 | INFO | seq2seq | validation loss:\t4.5411\n",
      "2021-02-04 15:22:44 | INFO | seq2seq | BLEU = 13.34 44.9/20.5/10.1/5.3 (BP = 0.897 ratio = 0.902 hyp_len = 87260 ref_len = 96725)\n",
      "2021-02-04 15:22:44 | INFO | seq2seq | saved epoch checkpoint:\t/home/george/Projects/mlta2021/checkpoints-transformer/checkpoint29.pt\n",
      "2021-02-04 15:22:44 | INFO | seq2seq | end of epoch 29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 30:   0%|          | 0/660 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:24:06 | INFO | seq2seq | training loss:\t4.7385\n",
      "2021-02-04 15:24:06 | INFO | seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:24:26 | INFO | seq2seq | example source: she was admitted to the hospital, where an ultrasound confirmed what we already suspected: her heart had weakened to less than half its normal capacity and had ballooned into the distinctive shape of a takotsubo.\n",
      "2021-02-04 15:24:26 | INFO | seq2seq | example hypothesis: 她必須承認醫院,在醫院裡,我們已經懷疑了:她的心靈感感感:她的心臟有一半正常的反應,除了一半正常的標準。\n",
      "2021-02-04 15:24:26 | INFO | seq2seq | example reference: 她住院了,在醫院,超音波確認了我們的懷疑:她的心臟變弱了,只剩不到一半的正常能力,且鼓起來成為章魚壺的形狀。\n",
      "2021-02-04 15:24:26 | INFO | seq2seq | validation loss:\t4.4943\n",
      "2021-02-04 15:24:26 | INFO | seq2seq | BLEU = 13.66 46.1/21.4/10.7/5.6 (BP = 0.876 ratio = 0.883 hyp_len = 85412 ref_len = 96725)\n",
      "2021-02-04 15:24:26 | INFO | seq2seq | saved epoch checkpoint:\t/home/george/Projects/mlta2021/checkpoints-transformer/checkpoint30.pt\n",
      "2021-02-04 15:24:26 | INFO | seq2seq | end of epoch 30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 31:   0%|          | 0/660 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:25:48 | INFO | seq2seq | training loss:\t4.6874\n",
      "2021-02-04 15:25:48 | INFO | seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:26:08 | INFO | seq2seq | example source: now, cavefishes can tell me a lot about biology and geology.\n",
      "2021-02-04 15:26:08 | INFO | seq2seq | example hypothesis: 克里斯可以告訴我生物學和喬治學。\n",
      "2021-02-04 15:26:08 | INFO | seq2seq | example reference: 洞穴魚其實可以告訴我們許多生物學與地質學的故事。\n",
      "2021-02-04 15:26:08 | INFO | seq2seq | validation loss:\t4.4500\n",
      "2021-02-04 15:26:08 | INFO | seq2seq | BLEU = 14.26 47.9/22.6/11.4/6.0 (BP = 0.865 ratio = 0.873 hyp_len = 84480 ref_len = 96725)\n",
      "2021-02-04 15:26:08 | INFO | seq2seq | saved epoch checkpoint:\t/home/george/Projects/mlta2021/checkpoints-transformer/checkpoint31.pt\n",
      "2021-02-04 15:26:08 | INFO | seq2seq | end of epoch 31\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 32:   0%|          | 0/660 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:27:30 | INFO | seq2seq | training loss:\t4.6446\n",
      "2021-02-04 15:27:30 | INFO | seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:27:51 | INFO | seq2seq | example source: i saw chains of jellyfish called siphonophores that were longer than this room, pumping out so much light that i could read the dials and gauges inside the suit without a flashlight; and puffs and billows of what looked like luminous blue smoke; and explosions of sparks that would swirl up out of the thrusters -- just like when you throw a log on a campfire and the embers swirl up off the campfire, but these were icy, blue embers.\n",
      "2021-02-04 15:27:51 | INFO | seq2seq | example hypothesis: 我看到了jyyyyyyyyyyyy的椅子,希望我能閱讀數位的光線,光是沒有閃亮的,光亮亮的藍色,看起來像藍色的藍色,紅色的藍色的藍色。\n",
      "2021-02-04 15:27:51 | INFO | seq2seq | example reference: 我看到一連串稱為「管水母」的生物排成的水母鏈比這個房間還長發出的光之多即使在潛水裝裡不開手電筒我也看得到刻度跟儀表板的數字還有一陣陣大小翻騰看起來像藍色螢光的煙霧或是像從推進器渦旋而出蹦開的火花就像你丟一塊木頭到營火裡,火花霹啪往上竄,只不過這些是冰冷的藍色火光\n",
      "2021-02-04 15:27:51 | INFO | seq2seq | validation loss:\t4.4020\n",
      "2021-02-04 15:27:51 | INFO | seq2seq | BLEU = 14.60 47.5/22.6/11.5/6.1 (BP = 0.881 ratio = 0.887 hyp_len = 85824 ref_len = 96725)\n",
      "2021-02-04 15:27:51 | INFO | seq2seq | saved epoch checkpoint:\t/home/george/Projects/mlta2021/checkpoints-transformer/checkpoint32.pt\n",
      "2021-02-04 15:27:51 | INFO | seq2seq | end of epoch 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 33:   0%|          | 0/660 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:29:13 | INFO | seq2seq | training loss:\t4.6083\n",
      "2021-02-04 15:29:13 | INFO | seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:29:33 | INFO | seq2seq | example source: gh: it turns out putting diapers on your head and play-fighting until the kids fall asleep is a great way to love your kids.\n",
      "2021-02-04 15:29:33 | INFO | seq2seq | example hypothesis: 克:結果發現在你的頭和玩耍,直到孩子睡眠時,睡眠是很棒的方式。\n",
      "2021-02-04 15:29:33 | INFO | seq2seq | example reference: 亨利:事實證明,頭戴尿布,打打鬧鬧,玩到孩子們入睡為止,是愛你孩子的好方法。\n",
      "2021-02-04 15:29:33 | INFO | seq2seq | validation loss:\t4.3664\n",
      "2021-02-04 15:29:33 | INFO | seq2seq | BLEU = 15.09 49.3/23.6/12.1/6.5 (BP = 0.867 ratio = 0.875 hyp_len = 84639 ref_len = 96725)\n",
      "2021-02-04 15:29:33 | INFO | seq2seq | saved epoch checkpoint:\t/home/george/Projects/mlta2021/checkpoints-transformer/checkpoint33.pt\n",
      "2021-02-04 15:29:33 | INFO | seq2seq | end of epoch 33\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 34:   0%|          | 0/660 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:30:55 | INFO | seq2seq | training loss:\t4.5749\n",
      "2021-02-04 15:30:55 | INFO | seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:31:15 | INFO | seq2seq | example source: there are many eyes to spot danger.\n",
      "2021-02-04 15:31:15 | INFO | seq2seq | example hypothesis: 許多眼睛有許多眼睛。\n",
      "2021-02-04 15:31:15 | INFO | seq2seq | example reference: 有很多眼睛可以看到危險。\n",
      "2021-02-04 15:31:15 | INFO | seq2seq | validation loss:\t4.3337\n",
      "2021-02-04 15:31:15 | INFO | seq2seq | BLEU = 15.59 49.6/23.9/12.4/6.7 (BP = 0.881 ratio = 0.887 hyp_len = 85833 ref_len = 96725)\n",
      "2021-02-04 15:31:15 | INFO | seq2seq | saved epoch checkpoint:\t/home/george/Projects/mlta2021/checkpoints-transformer/checkpoint34.pt\n",
      "2021-02-04 15:31:15 | INFO | seq2seq | end of epoch 34\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 35:   0%|          | 0/660 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:32:37 | INFO | seq2seq | training loss:\t4.5443\n",
      "2021-02-04 15:32:37 | INFO | seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:32:58 | INFO | seq2seq | example source: very critically, how do we protect it, if we find it, and not contaminate it?\n",
      "2021-02-04 15:32:58 | INFO | seq2seq | example hypothesis: 非常重要的是,我們要如何保護它,如果我們找到它,不是把它打算呢?\n",
      "2021-02-04 15:32:58 | INFO | seq2seq | example reference: 非常關鍵的一點是,當我們發現生命了之後,應該要怎樣保護它並使它免於污染?\n",
      "2021-02-04 15:32:58 | INFO | seq2seq | validation loss:\t4.3010\n",
      "2021-02-04 15:32:58 | INFO | seq2seq | BLEU = 16.22 49.4/24.0/12.5/6.9 (BP = 0.909 ratio = 0.913 hyp_len = 88271 ref_len = 96725)\n",
      "2021-02-04 15:32:58 | INFO | seq2seq | saved epoch checkpoint:\t/home/george/Projects/mlta2021/checkpoints-transformer/checkpoint35.pt\n",
      "2021-02-04 15:32:58 | INFO | seq2seq | end of epoch 35\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 36:   0%|          | 0/660 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:34:20 | INFO | seq2seq | training loss:\t4.5160\n",
      "2021-02-04 15:34:20 | INFO | seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:34:41 | INFO | seq2seq | example source: i found that with whatever risk or gamble i took in putting my face out there, it was well worth any negative comment, any flak i received, because i felt i was able to make this real and this tangible impact.\n",
      "2021-02-04 15:34:41 | INFO | seq2seq | example hypothesis: 我發現,有無論風險,或是我把我的臉上放出來,它很糟糕,任何負面的衝擊,因為我覺得我可以讓這個真正的影響。\n",
      "2021-02-04 15:34:41 | INFO | seq2seq | example reference: 我發現,當我公開我的臉孔,不論我冒什麼風險或賭上什麼,任何負面評論都值得,我受到的任何抨擊都值得,因為我覺得我能夠造成這真實、有形的影響。\n",
      "2021-02-04 15:34:41 | INFO | seq2seq | validation loss:\t4.2742\n",
      "2021-02-04 15:34:41 | INFO | seq2seq | BLEU = 16.15 51.1/24.9/13.0/7.1 (BP = 0.871 ratio = 0.879 hyp_len = 85006 ref_len = 96725)\n",
      "2021-02-04 15:34:41 | INFO | seq2seq | saved epoch checkpoint:\t/home/george/Projects/mlta2021/checkpoints-transformer/checkpoint36.pt\n",
      "2021-02-04 15:34:41 | INFO | seq2seq | end of epoch 36\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 37:   0%|          | 0/660 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:36:03 | INFO | seq2seq | training loss:\t4.4934\n",
      "2021-02-04 15:36:03 | INFO | seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:36:23 | INFO | seq2seq | example source: my parents were fanatics about learning, and i'll come back to that a little bit later.\n",
      "2021-02-04 15:36:23 | INFO | seq2seq | example hypothesis: 我的父母很熟悉學習,我會回來這一點。\n",
      "2021-02-04 15:36:23 | INFO | seq2seq | example reference: 我父母非常熱衷學習關於這,我以後再來説\n",
      "2021-02-04 15:36:23 | INFO | seq2seq | validation loss:\t4.2529\n",
      "2021-02-04 15:36:23 | INFO | seq2seq | BLEU = 16.73 50.2/24.6/12.9/7.1 (BP = 0.912 ratio = 0.916 hyp_len = 88586 ref_len = 96725)\n",
      "2021-02-04 15:36:23 | INFO | seq2seq | saved epoch checkpoint:\t/home/george/Projects/mlta2021/checkpoints-transformer/checkpoint37.pt\n",
      "2021-02-04 15:36:23 | INFO | seq2seq | end of epoch 37\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 38:   0%|          | 0/660 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:37:45 | INFO | seq2seq | training loss:\t4.4724\n",
      "2021-02-04 15:37:45 | INFO | seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:38:03 | INFO | seq2seq | example source: the educated blamed the less well-educated.\n",
      "2021-02-04 15:38:03 | INFO | seq2seq | example hypothesis: 教育的廣告是較少的。\n",
      "2021-02-04 15:38:03 | INFO | seq2seq | example reference: 知識分子指責未受良好教育的人。\n",
      "2021-02-04 15:38:03 | INFO | seq2seq | validation loss:\t4.2432\n",
      "2021-02-04 15:38:03 | INFO | seq2seq | BLEU = 16.56 52.7/26.1/13.8/7.7 (BP = 0.846 ratio = 0.856 hyp_len = 82831 ref_len = 96725)\n",
      "2021-02-04 15:38:03 | INFO | seq2seq | saved epoch checkpoint:\t/home/george/Projects/mlta2021/checkpoints-transformer/checkpoint38.pt\n",
      "2021-02-04 15:38:03 | INFO | seq2seq | end of epoch 38\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 39:   0%|          | 0/660 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:39:25 | INFO | seq2seq | training loss:\t4.4555\n",
      "2021-02-04 15:39:25 | INFO | seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:39:44 | INFO | seq2seq | example source: my announcement was just two lines in a chat channel.\n",
      "2021-02-04 15:39:44 | INFO | seq2seq | example hypothesis: 我的創意只有兩條條線。\n",
      "2021-02-04 15:39:44 | INFO | seq2seq | example reference: 我只是在一個聊天管道上寫了兩行文字。\n",
      "2021-02-04 15:39:44 | INFO | seq2seq | validation loss:\t4.2345\n",
      "2021-02-04 15:39:44 | INFO | seq2seq | BLEU = 16.24 53.7/26.8/14.3/7.9 (BP = 0.808 ratio = 0.824 hyp_len = 79713 ref_len = 96725)\n",
      "2021-02-04 15:39:44 | INFO | seq2seq | saved epoch checkpoint:\t/home/george/Projects/mlta2021/checkpoints-transformer/checkpoint39.pt\n",
      "2021-02-04 15:39:44 | INFO | seq2seq | end of epoch 39\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 40:   0%|          | 0/660 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:41:06 | INFO | seq2seq | training loss:\t4.4412\n",
      "2021-02-04 15:41:06 | INFO | seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:41:26 | INFO | seq2seq | example source: and what these women understood is sometimes the most important things that we do and that we spend our time on are those things that we cannot measure.\n",
      "2021-02-04 15:41:26 | INFO | seq2seq | example hypothesis: 而這些女性了解的是,有時我們所做的最重要的事,我們花了時間,是我們無法測量的事物。\n",
      "2021-02-04 15:41:26 | INFO | seq2seq | example reference: 這些婦女明白到有時候我們做最重要的事和我們花上最多的時間就是那些我們無法衡量的東西。\n",
      "2021-02-04 15:41:26 | INFO | seq2seq | validation loss:\t4.2022\n",
      "2021-02-04 15:41:26 | INFO | seq2seq | BLEU = 17.12 52.5/26.1/13.9/7.8 (BP = 0.872 ratio = 0.879 hyp_len = 85035 ref_len = 96725)\n",
      "2021-02-04 15:41:26 | INFO | seq2seq | saved epoch checkpoint:\t/home/george/Projects/mlta2021/checkpoints-transformer/checkpoint40.pt\n",
      "2021-02-04 15:41:26 | INFO | seq2seq | end of epoch 40\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"task: {}\".format(task.__class__.__name__))\n",
    "logger.info(\"model: {}\".format(model.__class__.__name__))\n",
    "logger.info(\"criterion: {}\".format(criterion.__class__.__name__))\n",
    "logger.info(\"optimizer: {}\".format(optimizer.__class__.__name__))\n",
    "logger.info(\n",
    "    \"num. model params: {:,} (num. trained: {:,})\".format(\n",
    "        sum(p.numel() for p in model.parameters()),\n",
    "        sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "    )\n",
    ")\n",
    "logger.info(f\"max tokens per batch = {config.max_tokens}, accumulate steps = {config.accum_steps}\")\n",
    "epoch_itr = load_data_iterator(task, \"train\", config.start_epoch, config.max_tokens, config.num_workers)\n",
    "try_load_checkpoint(model, name=config.resume)\n",
    "while epoch_itr.next_epoch_idx <= config.max_epoch:\n",
    "    # train for one epoch\n",
    "    train_one_epoch(epoch_itr, model, task, criterion, optimizer, config.accum_steps)\n",
    "    stats = validate_and_save(model, task, criterion, epoch=epoch_itr.epoch)\n",
    "    logger.info(\"end of epoch {}\".format(epoch_itr.epoch))    \n",
    "    epoch_itr = load_data_iterator(task, \"train\", epoch_itr.next_epoch_idx, config.max_tokens, config.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'checkpoints-rnn': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls checkpoints-rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path(savedir).mkdir(parents=True, exist_ok=True)\n",
    "# torch.save(model.state_dict(), Path(savedir)/\"checkpoint99.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(Path(savedir)/\"checkpoint19.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate_and_save(model, task, criterion, epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate_and_save(model, task, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check['args'].langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fairseq.tasks.translation_from_pretrained_bart import TranslationFromPretrainedBARTTask\n",
    "# task_cfg = TranslationConfig(\n",
    "#     data=datadir,\n",
    "#     source_lang=src,\n",
    "#     target_lang=tgt,\n",
    "#     train_subset=\"train\",\n",
    "#     required_seq_len_multiple=8,\n",
    "#     dataset_impl=\"mmap\",\n",
    "# )\n",
    "# task_cfg = Namespace(\n",
    "#     **vars(task_cfg),\n",
    "#     langs=\"en_XX,zh_CN\",\n",
    "#     prepend_bos=True\n",
    "# )\n",
    "# task = TranslationFromPretrainedBARTTask.setup_task(task_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 成對雙語言資料\n",
    "* iwslt17\n",
    "* UM-Corpus的子集\n",
    "    - 新聞: 450,000句\n",
    "    - 口語: 220,000句\n",
    "    - 教育: 450,000句\n",
    "    - 字幕: 300,000 (含TED)\n",
    "* TED2020 http://opus.nlpl.eu/TED2020-v1.php, \n",
    "    - 原資料量: 404,726句\n",
    "    - 訓練資料: 335,785句\n",
    "* OpenSubtitles http://opus.nlpl.eu/OpenSubtitles-v2018.php, http://www.opensubtitles.org/\n",
    "    - 訓練資料: 4,772,273句\n",
    "    \n",
    "J. Tiedemann, 2012, Parallel Data, Tools and Interfaces in OPUS. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC 2012)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文單語言資料\n",
    "- newscrawl資料集 (wmt19提供):\n",
    "  - 2018訓練資料：589,475句\n",
    "  - 2019訓練資料：2,974,040句\n",
    "    \n",
    "- 語言\n",
    "  - 繁體中文: news.201X.zh.shuffled.deduped.trad (使用opencc轉換 簡->繁)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.target_dictionary.eos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "itr = load_data_iterator(task, \"valid\", 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  40, 1245,  229, 1262,    2,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['target'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,   40, 1245,  229, 1262,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['net_input']['prev_output_tokens'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mltatest]",
   "language": "python",
   "name": "conda-env-mltatest-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
